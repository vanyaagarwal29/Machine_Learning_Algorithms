{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMINN+ShQyktObTLh7T/PPq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vanyaagarwal29/Machine_Learning_Algorithms/blob/main/Machine_Learning_Assignment_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzqUipEn7VXU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is feature engineering, and how does it work? Explain the various aspects of feature\n",
        "engineering in depth.\n",
        "2. What is feature selection, and how does it work? What is the aim of it? What are the various\n",
        "methods of function selection?\n",
        "3. Describe the function selection filter and wrapper approaches. State the pros and cons of each\n",
        "approach?\n",
        "\n",
        "4.\n",
        "\n",
        "i. Describe the overall feature selection process.\n",
        "\n",
        "ii. Explain the key underlying principle of feature extraction using an example. What are the most\n",
        "widely used function extraction algorithms?\n",
        "\n",
        "5. Describe the feature engineering process in the sense of a text categorization issue.\n",
        "\n",
        "6. What makes cosine similarity a good metric for text categorization? A document-term matrix has\n",
        "two rows with values of (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1). Find the resemblance in\n",
        "cosine.\n",
        "\n",
        "7.\n",
        "\n",
        "i. What is the formula for calculating Hamming distance? Between 10001011 and 11001111,\n",
        "calculate the Hamming gap.\n",
        "\n",
        "ii. Compare the Jaccard index and similarity matching coefficient of two features with values (1, 1, 0,\n",
        "0, 1, 0, 1, 1) and (1, 1, 0, 0, 0, 1, 1, 1), respectively (1, 0, 0, 1, 1, 0, 0, 1).\n",
        "\n",
        "8. State what is meant by &quot;high-dimensional data set&quot;? Could you offer a few real-life examples?\n",
        "What are the difficulties in using machine learning techniques on a data set with many dimensions?\n",
        "What can be done about it?\n",
        "\n",
        "9. Make a few quick notes on:\n",
        "\n",
        "PCA is an acronym for Personal Computer Analysis.\n",
        "\n",
        "2. Use of vectors\n",
        "\n",
        "3. Embedded technique\n",
        "\n",
        "10. Make a comparison between:\n",
        "\n",
        "1. Sequential backward exclusion vs. sequential forward selection\n",
        "\n",
        "2. Function selection methods: filter vs. wrapper\n",
        "\n",
        "3. SMC vs. Jaccard coefficient"
      ],
      "metadata": {
        "id": "qBC5fcwo7WRT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Feature engineering is the process of transforming and creating new features from raw data to improve the performance of machine learning models. It involves selecting, combining, and transforming data features to make them more suitable for the learning algorithm. The main aspects of feature engineering include:\n",
        "\n",
        "   a. Handling Missing Data: Dealing with missing values in the dataset, either by imputation or creating new features to capture the absence of data.\n",
        "   b. Encoding Categorical Variables: Converting categorical variables into numerical representations that machine learning algorithms can handle.\n",
        "   c. Scaling and Normalization: Scaling features to a similar range to prevent any particular feature from dominating the learning process.\n",
        "   d. Creating Interaction Features: Generating new features by combining existing ones to capture complex relationships between variables.\n",
        "   e. Feature Transformation: Applying mathematical functions (e.g., logarithm, square root) to transform features and make the data distribution more suitable for modeling.\n",
        "\n",
        "2. Feature selection is the process of selecting a subset of the most relevant features from the original feature set to reduce the dimensionality and improve model performance. The aim is to eliminate irrelevant or redundant features that may cause overfitting or slow down the learning process. Various methods of feature selection include:\n",
        "\n",
        "   a. Filter Methods: These methods evaluate the relevance of features independently of the learning algorithm. Common techniques include correlation analysis, chi-square test, and mutual information-based measures.\n",
        "   b. Wrapper Methods: These methods use the learning algorithm's performance as a criterion for selecting features. Techniques like recursive feature elimination (RFE) and forward/backward feature selection fall under this category.\n",
        "\n",
        "3. Filter Approach Pros and Cons:\n",
        "   - Pros: Faster computation as features are evaluated independently, less prone to overfitting.\n",
        "   - Cons: Ignores feature dependencies, may not perform well if features are highly correlated.\n",
        "\n",
        "   Wrapper Approach Pros and Cons:\n",
        "   - Pros: Considers feature dependencies, can lead to better model performance.\n",
        "   - Cons: More computationally expensive, prone to overfitting on the training set.\n",
        "\n",
        "4. i. Overall Feature Selection Process:\n",
        "   - Preprocess the data by handling missing values and encoding categorical variables.\n",
        "   - Apply feature selection techniques to rank or score features based on their relevance.\n",
        "   - Select the top-ranked features or use iterative methods like RFE to create a subset of features.\n",
        "   - Train the machine learning model using the selected features and evaluate its performance.\n",
        "\n",
        "   ii. Feature Extraction Principle Example:\n",
        "   Principal Component Analysis (PCA) is a widely used feature extraction algorithm that transforms high-dimensional data into a lower-dimensional space while preserving the maximum variance in the data. It creates new orthogonal features (principal components) that capture the most significant information from the original features.\n",
        "\n",
        "5. In text categorization, feature engineering involves converting raw text data into numerical representations suitable for machine learning models. Techniques like tokenization, stemming, and vectorization (e.g., TF-IDF) are used to represent text as numerical feature vectors.\n",
        "\n",
        "6. Cosine similarity is a metric commonly used for text categorization to measure the similarity between two text documents represented as vectors. Given the document-term matrix (DTM) rows (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1), the cosine similarity can be calculated as the dot product of the two vectors divided by the product of their magnitudes. The cosine similarity between the two vectors is approximately 0.73.\n",
        "\n",
        "7. i. Hamming distance is the number of positions at which two strings differ. Between 10001011 and 11001111, the Hamming distance is 3, as they differ at three positions: the 3rd, 5th, and 6th positions.\n",
        "\n",
        "   ii. Jaccard index and similarity matching coefficient measure the similarity between two sets. For the sets (1, 1, 0, 0, 1, 0, 1, 1) and (1, 1, 0, 0, 0, 1, 1, 1), the Jaccard index is 0.67, and the similarity matching coefficient is 0.75.\n",
        "\n",
        "8. High-dimensional data sets refer to data with a large number of features or dimensions. Real-life examples include gene expression data with thousands of genes, image data with pixels as features, or text data with word frequencies as features. The difficulties in using machine learning techniques on high-dimensional data include increased computational complexity, overfitting, and the curse of dimensionality. Techniques like feature selection, dimensionality reduction (e.g., PCA), and regularization can be used to mitigate these challenges.\n",
        "\n",
        "9. Quick notes:\n",
        "\n",
        "   a. PCA stands for Principal Component Analysis, a dimensionality reduction technique that transforms data into a lower-dimensional space.\n",
        "   b. Vectors are mathematical representations of data points or features in a multi-dimensional space.\n",
        "   c.\n",
        "\n",
        " Embedded techniques combine feature selection and model building to select relevant features during the learning process.\n",
        "\n",
        "10. Comparison:\n",
        "\n",
        "    i. Sequential backward exclusion vs. sequential forward selection: Both are feature selection methods. Backward exclusion starts with all features and eliminates them iteratively, while forward selection starts with an empty set and adds features iteratively.\n",
        "\n",
        "    ii. Function selection methods: filter vs. wrapper: Both are feature selection methods. Filter methods evaluate feature relevance independently of the learning algorithm, while wrapper methods use the learning algorithm's performance as a criterion for feature selection.\n",
        "\n",
        "    iii. SMC vs. Jaccard coefficient: Both are similarity metrics. SMC measures the similarity between two sets, considering the common elements divided by the union of elements. Jaccard coefficient measures the similarity between two sets, considering the common elements divided by the total elements (union minus common)."
      ],
      "metadata": {
        "id": "JWFFhl0_7Y0z"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ErH-OWAY7XHY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}