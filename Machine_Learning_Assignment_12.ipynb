{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4Nc1tSCPAHOCG8QT36WcX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vanyaagarwal29/Machine_Learning_Algorithms/blob/main/Machine_Learning_Assignment_12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWFy7dnq9i02"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Prior probability: Prior probability refers to the probability of an event occurring before considering any new evidence or information. It represents the initial belief or knowledge about the likelihood of an event. For example, if we are predicting the likelihood of rain tomorrow based on historical weather data, the prior probability of rain might be the average percentage of rainy days in a particular month.\n",
        "\n",
        "2. Posterior probability: Posterior probability refers to the updated probability of an event occurring after incorporating new evidence or information. It is obtained using Bayes' theorem, which takes into account both the prior probability and the likelihood of the new evidence. For example, if we initially predicted the probability of rain tomorrow based on historical data (prior probability) and then receive a weather report indicating certain weather patterns (new evidence), the updated probability of rain based on the combined information is the posterior probability.\n",
        "\n",
        "3. Likelihood probability: Likelihood probability refers to the probability of observing a particular set of evidence or data given a specific hypothesis or model. It is a measure of how well the evidence supports a particular hypothesis. For example, in a medical diagnosis scenario, the likelihood probability might represent the probability of certain symptoms being present given a particular disease.\n",
        "\n",
        "4. Naïve Bayes classifier: The Naïve Bayes classifier is a probabilistic machine learning algorithm based on Bayes' theorem. It is used for classification tasks and is named \"Naïve\" because of the assumption of independence between features. The algorithm assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. Despite this simplifying assumption, the Naïve Bayes classifier performs well in many real-world applications, especially in text classification tasks.\n",
        "\n",
        "5. Optimal Bayes classifier: The optimal Bayes classifier, also known as the Bayes optimal classifier, is a theoretical classifier that gives the best possible performance when the true underlying probability distributions are known. It is derived directly from Bayes' theorem and makes decisions based on the highest posterior probability. However, the optimal Bayes classifier is often infeasible in practice because the true probability distributions are typically unknown.\n",
        "\n",
        "6. Two features of Bayesian learning methods:\n",
        "   a. Bayesian methods incorporate prior knowledge or beliefs in the form of prior probabilities, which can be useful when there is some existing information about the problem being solved.\n",
        "   b. Bayesian learning provides a principled way to update beliefs based on new evidence, making it a powerful framework for handling uncertainty and making decisions.\n",
        "\n",
        "7. Consistent learners: Consistent learners are machine learning algorithms that converge to the true underlying model or target function as the amount of training data approaches infinity. In other words, consistent learners will produce more accurate predictions as the amount of data increases.\n",
        "\n",
        "8. Two strengths of Bayes classifier:\n",
        "   a. Simplicity: The Naïve Bayes classifier is easy to implement and computationally efficient, making it suitable for large-scale applications.\n",
        "   b. Robustness to irrelevant features: The Naïve Bayes classifier tends to perform well even when some of the features are irrelevant or redundant, thanks to the assumption of feature independence.\n",
        "\n",
        "9. Two weaknesses of Bayes classifier:\n",
        "   a. Strong independence assumption: The Naïve Bayes classifier assumes that all features are independent, which might not hold true in some real-world scenarios. This can lead to suboptimal performance in certain cases.\n",
        "   b. Data scarcity: The Naïve Bayes classifier relies on estimating probabilities from the data, and with limited data, it may produce less accurate predictions.\n",
        "\n",
        "10. How Naïve Bayes classifier is used for:\n",
        "    1. Text classification: In text classification, the Naïve Bayes classifier can be used to classify documents into predefined categories (e.g., spam vs. non-spam emails, sentiment analysis of customer reviews, topic classification of news articles) based on the occurrence of words or features in the text.\n",
        "    2. Spam filtering: Naïve Bayes is a popular algorithm for spam filtering, where it classifies emails as either spam or non-spam (ham) based on the presence of certain words or patterns in the email content.\n",
        "    3. Market sentiment analysis: Naïve Bayes can be used in market sentiment analysis to classify financial news or social media posts into positive, negative, or neutral sentiment categories, helping investors make decisions based on public sentiment towards certain assets or companies."
      ],
      "metadata": {
        "id": "BMAp7vJG9l5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JAWzC_v8958F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}